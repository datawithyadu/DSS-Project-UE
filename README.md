ğŸ  House Price Prediction using Random Forest & XGBoost
ğŸ“Œ Project Overview

This project develops a supervised machine learning regression model to predict residential house prices using the Ames Housing dataset.

Unlike a basic modeling approach, this project emphasizes:

Structured preprocessing pipeline

Feature relevance analysis using Mutual Information

Cross-model feature importance comparison

Dimensionality reduction via zero-importance filtering

Hyperparameter optimization using GridSearchCV

Source Notebook: 

house_price_prediction_xg (6)

ğŸ“Š Dataset

Dataset: Ames Housing (Kaggle Competition)

79 explanatory variables

Target variable: SalePrice

The dataset includes property attributes such as:

Construction quality

Living area

Basement area

Garage capacity

Neighborhood

Sale conditions

ğŸ§  Machine Learning Pipeline
1ï¸âƒ£ Data Preprocessing

Dropped ID column

Separated numerical & categorical features

Imputed:

Numerical â†’ Mean strategy

Categorical â†’ Most frequent strategy

Applied OneHotEncoding via ColumnTransformer

2ï¸âƒ£ Feature Relevance Analysis

To reduce noise and improve model efficiency:

Applied Mutual Information Regression

Extracted:

Random Forest feature importance

XGBoost feature importance

Removed:

Features with XGBoost importance = 0

Features with RF importance = 0

Combined redundant feature lists

This reduced dimensionality while maintaining predictive power.

3ï¸âƒ£ Model Comparison
ğŸ”¹ Random Forest Regressor

n_estimators = 200

RMSE â‰ˆ 33,107

ğŸ”¹ XGBoost (Initial)

Default parameters

Validation RMSE â‰ˆ 32,003

Training RMSE â‰ˆ 1,400
â†’ Identified overfitting

4ï¸âƒ£ Feature Filtering & Retraining

After removing zero-contributing features:

Random Forest RMSE â‰ˆ 33,238

XGBoost improved performance

5ï¸âƒ£ Hyperparameter Tuning

Used GridSearchCV:

Parameters tuned:

n_estimators

max_depth

learning_rate

Cross-validation: 3-fold
Scoring: Negative MSE

Final optimized model selected via best RMSE.

ğŸ“ˆ Final Performance

Final optimized XGBoost model achieved:

RMSE â‰ˆ 27,839

Improvement achieved through:

Feature elimination

Controlled model complexity

Cross-validated tuning

ğŸ† Key Technical Takeaways

Feature selection via model-based importance can reduce dimensionality without degrading performance.

XGBoost outperforms Random Forest for structured tabular regression tasks.

Hyperparameter tuning significantly reduces overfitting.

Proper preprocessing is critical in tabular ML pipelines.

ğŸ›  Tech Stack

Python

Pandas

NumPy

Matplotlib / Seaborn

Scikit-learn

XGBoost

ğŸ”¥ Now â€” Strategic LinkedIn Positioning

You SHOULD:

âœ… Add under â€œProjectsâ€
âœ… Create a LinkedIn post
âŒ Do NOT add as fake "Machine Learning Engineer Experience"

You're still transitioning â€” authenticity builds stronger trust.

ğŸš€ LinkedIn Post Template (Authority Style)

You can post something like this:

ğŸ“Š From Data to Decisions: Predicting House Prices with Machine Learning

I recently built an end-to-end regression pipeline to predict residential house prices using the Ames Housing dataset.

Instead of stopping at model training, I:

â€¢ Performed feature relevance analysis using Mutual Information
â€¢ Compared Random Forest and XGBoost
â€¢ Eliminated zero-contributing features
â€¢ Applied GridSearchCV for hyperparameter optimization
â€¢ Reduced RMSE to 27,839

This project strengthened my understanding of:

âœ” Model interpretability
âœ” Overfitting detection
âœ” Feature importance comparison
âœ” Performance optimization in structured datasets

GitHub link: [your link]

Excited to keep building in ML & AI ğŸš€

