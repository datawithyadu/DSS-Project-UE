# ğŸ  House Price Prediction using Random Forest & XGBoost

## ğŸ“Œ Project Overview

This project develops an end-to-end supervised machine learning regression model to predict residential house prices using the Ames Housing dataset (Kaggle Competition).

The objective is to build a robust predictive model capable of estimating **SalePrice** based on 79 structured property features such as construction quality, living area, basement size, garage capacity, and neighborhood.

Unlike a basic modeling approach, this project emphasizes:

- Structured preprocessing pipeline  
- Feature relevance analysis using Mutual Information  
- Cross-model feature importance comparison  
- Dimensionality reduction via zero-importance filtering  
- Hyperparameter optimization using GridSearchCV  

---

## ğŸ“Š Dataset

- Source: [Ames Housing Dataset - Kaggle](https://www.kaggle.com/competitions/home-data-for-ml-course/overview)
- Target Variable: `SalePrice`
- 79 explanatory variables

---

## ğŸ” Machine Learning Pipeline

### 1ï¸âƒ£ Data Preprocessing
- Removed ID column
- Separated numerical and categorical features
- Missing value treatment:
  - Numerical â†’ Mean Imputation
  - Categorical â†’ Most Frequent Imputation
- Applied OneHotEncoding using ColumnTransformer
- Verified absence of null values

---

### 2ï¸âƒ£ Feature Relevance Analysis

- Applied **Mutual Information Regression**
- Extracted feature importance from:
  - Random Forest
  - XGBoost
- Removed zero-importance features
- Reduced dimensionality without degrading performance

---

### 3ï¸âƒ£ Model Comparison

#### ğŸ”¹ Random Forest Regressor
- n_estimators = 200
- Validation RMSE â‰ˆ 33,107

#### ğŸ”¹ XGBoost (Initial Model)
- Default configuration
- Validation RMSE â‰ˆ 32,003
- Training RMSE â‰ˆ 1,400 (Overfitting observed)

---

### 4ï¸âƒ£ Hyperparameter Tuning

Used **GridSearchCV (3-fold cross-validation)**

Parameters tuned:
- n_estimators
- max_depth
- learning_rate

Scoring metric:
- Negative Mean Squared Error

---

## ğŸ“ˆ Final Performance

Final Optimized XGBoost Model:

**RMSE â‰ˆ 27,839**

Performance improved through:
- Feature elimination
- Controlled model complexity
- Cross-validated hyperparameter tuning

---

## ğŸ›  Tech Stack

- Python  
- Pandas  
- NumPy  
- Matplotlib  
- Seaborn  
- Scikit-learn  
- XGBoost  

---

## ğŸ“‚ Source Notebook

[Open Notebook](house_price_prediction_xg.ipynb)

---

## ğŸš€ How to Run

1. Clone the repository  
   ```
   git clone https://github.com/yourusername/your-repo-name.git
   ```

2. Install dependencies  
   ```
   pip install -r requirements.txt
   ```

3. Run the notebook  
   ```
   jupyter notebook
   ```

---

## ğŸ“Œ Author

**Yadukrishnan**  
Aspiring Data Scientist | Machine Learning & AI Enthusiast
